{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data=pd.read_csv(\"C:\\\\Users\\\\Admin\\\\Desktop\\\\FakeNews\\\\news.csv\")  ## File Location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the training data:{}\".format(news_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data['title'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data.dropna(how='any',subset=['title'],inplace=True)   ## We will be processing title to check authenticity of news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The shape of the training data after the null values in title are removed:{}\".format(news_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label=news_data['label']\n",
    "label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reliable_news_count=label.value_counts()[0]\n",
    "unreliable_news_count=label.value_counts()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(y_train))\n",
    "class_plot=[reliable_news_count,unreliable_news_count]\n",
    "plt.pie(class_plot,labels=['Reliable Article','Unreliable Article'],shadow=True,wedgeprops={'edgecolor':'black'},autopct='%1.1f%%',startangle=90,textprops={'fontsize':18})\n",
    "plt.suptitle('Label Distribution',fontsize=25)\n",
    "plt.tight_layout()\n",
    "plt.style.use('ggplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bar=['0','1']\n",
    "plt.bar(X_bar,class_plot,color=['blue','green'])\n",
    "plt.xlabel('Labels',fontsize=15)\n",
    "plt.ylabel('Frequency',fontsize=15)\n",
    "plt.suptitle('Class Distribution',size=30)\n",
    "plt.tight_layout()\n",
    "plt.grid(False)\n",
    "plt.style.use('seaborn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news=news_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_headlines=news_data['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserting important NLP libraries\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "ps=PorterStemmer()\n",
    "news_corpus=[]\n",
    "for i in range(0,len(news_headlines)):\n",
    "    \n",
    "    news_data=re.sub('[^a-zA-Z]',' ',news_headlines[i])\n",
    "    news_data=news_data.lower()\n",
    "    news_data=news_data.split()\n",
    "    news_data=[ps.stem(word) for word in news_data if word not in set(stopwords.words('english'))]\n",
    "    news_data=' '.join(news_data)\n",
    "    news_corpus.append(news_data)\n",
    "    count=count+1\n",
    "    if (count%100==0):\n",
    "        print(\"{} data headlines processed\".format(count))\n",
    "        pct_complete=(count/len(news_headlines))*100\n",
    "        pct_complete=round(pct_complete,2)\n",
    "        pct_left=100-pct_complete\n",
    "        print(\"{}% processing left\".format(pct_left))\n",
    "        \n",
    "    elif (count==len(news_headlines)):\n",
    "        \n",
    "        print(\"Text Preprocessing complete\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting number of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_words=[]\n",
    "word_count=0\n",
    "for i in news_corpus:\n",
    "    \n",
    "    news_data=i.split()\n",
    "    for j in news_data:\n",
    "        \n",
    "        if j not in news_words:\n",
    "            \n",
    "            word_count=word_count+1\n",
    "            news_words.append(j)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            pass\n",
    "        \n",
    "print(\"The number of unique words in the training set: {}\".format(word_count))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=15000\n",
    "oov_token = '<UNK>'\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing our training data\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
    "tokenizer.fit_on_texts(news_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dictionary = tokenizer.word_index\n",
    "print(word_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_encoded_data = tokenizer.texts_to_sequences(news_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(news_encoded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding the news input to a fixed length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlength_list=[]\n",
    "for x in news_encoded_data:\n",
    "    \n",
    "    length_x=len(x)\n",
    "    maxlength_list.append(length_x)\n",
    "    \n",
    "MAXLEN=max(maxlength_list)\n",
    "print(\"Minimum Padding length required:{}\".format(MAXLEN)) ## So as to fit the largest possible input from training while padding\n",
    "## print(len(maxlength_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_length=MAXLEN\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_news_vector=pad_sequences(news_encoded_data,padding='post',maxlen=sentence_length)\n",
    "print(\"Padding completed for {} news inputs\".format(len(padded_news_vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(padded_news_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_embedded_dims=120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Embedding(vocab_size,output_embedded_dims,input_length=sentence_length))\n",
    "    model.add(Bidirectional(LSTM(units=200,return_sequences=True)))\n",
    "    model.add(Dropout(rate=0.3))\n",
    "    model.add(Bidirectional(LSTM(units=200,return_sequences=True)))\n",
    "    model.add(Dropout(rate=0.3))\n",
    "    model.add(Bidirectional(LSTM(units=200,return_sequences=True)))\n",
    "    model.add(Dropout(rate=0.3))\n",
    "    model.add(Bidirectional(LSTM(units=200,return_sequences=False)))\n",
    "    model.add(Dense(units=200,activation='relu'))\n",
    "    model.add(Dropout(rate=0.3))\n",
    "    model.add(Dense(units=100,activation='relu'))\n",
    "    model.add(Dropout(rate=0.3))\n",
    "    model.add(Dense(units=100,activation='relu'))\n",
    "    model.add(Dense(units=1,activation='sigmoid')) # Adding an output layer\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_predictor=create_model()\n",
    "news_predictor.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(news_predictor.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(padded_news_vector)\n",
    "y=np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_predictor.fit(X,y,batch_size=256,epochs=10,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_predictor.save('news_predictor.h5')      ## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_predictor.save_weights('news_predictor_weights.h5')   ## Saving the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('vocabulary.pkl','wb') as vocab_file:  ## Saving the vocabulary\n",
    "    \n",
    "    pickle.dump(news_corpus,vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pkl','wb') as token_file:\n",
    "    \n",
    "    pickle.dump(tokenizer,token_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
